betaEffect <- rep(10, K)
if (design=="staggered_adoption"){
observed.params <- parameter_maker_staggered(N=N, Time=Time, propTreat=propTreat,
linkFun='logit', K=K, lengthCutoff=lengthCutoff,
qFunc=qexp, qFuncParms = list(2))
# norm(observed.params$trueWeightMatrix-observed.params$weightMatrixEst, 'F')/sqrt(prod(dim(observed.params$weightMatrixEst)))
norm(observed.params$trueWeightMatrix-observed.params$weightMatrixEst, 'F')/sqrt(prod(dim(observed.params$weightMatrixEst)))
xBeta <- covariateEffectComponent(covArray=observed.params$covArray, betaEffect)
W <- observed.params$W
}else if (design=="block_treatment"){
# mean=c(-.5*group_gap, .5*group_gap)
observed.params <- parameter.maker_block(N=N, Time=Time, Time0=Time0, propTreat=propTreat,
link_func=link_func,
dist=rexp, prob_func = prob_func)
xBeta <- covariateEffectComponent(covArray=observed.params$covArray, betaEffect)
W <- observed.params$W
}
final.weight.matrix <- observed.params$weightMatrixEst
tau_matrix <- t(apply(W, MARGIN=1, FUN=treated_matrix_creator,
f_of_t=treatment_function, arg_max=arg_max,
y_max=y_max, halfway_time=halfway_time, cutoff=cutoff, value=tau))
control_units <- as.numeric(which(apply(W, MARGIN=1, FUN = function(x) all(x==0))))
pretreatment_times <- as.numeric(which(apply(W, MARGIN=2, FUN = function(x) all(x==0))))
treated_units <- as.numeric(which(apply(W, MARGIN=1, FUN = function(x) any(x==1))))
treatment_times <- as.numeric(which(apply(W, MARGIN=2, FUN = function(x) any(x==1))))
W_document_missing <- W
delta_t <- treatment_function(treatment_times-(min(treatment_times)-1),
arg_max=arg_max,
y_max=y_max, halfway_time=halfway_time, cutoff=cutoff, value=tau)
prediction_error_matrix_mc_nnm <- matrix(NA, nrow=1, ncol=number_of_Es)
prediction_error_matrix_r1comp <- matrix(NA, nrow=1, ncol=number_of_Es)
prediction_error_matrix_oracle <- matrix(NA, nrow=1, ncol=number_of_Es)
autocorrelation_matrix <- make_rho_mat(rho=rho_parameter, p=dim(W)[2])
sig_to_noise_ratios <- c()
desired_clusters <- 30
max_available_clusters <- detectCores()
cl <- makeCluster(min(c(max_available_clusters, desired_clusters))-3)
registerDoParallel(cl)
### Single run comparison
# set.seed(3729)
errors_this_L_mc_nnm <- rep(NA, number_of_Es)
errors_this_L_r1comp <- rep(NA, number_of_Es)
errors_this_L_oracle <- rep(NA, number_of_Es)
L <- matrix(5, nrow=N, ncol=Time)
for (j in 1:number_of_Es){
print(j)
Y <- nortaNoise(number=N, corr_mat=autocorrelation_matrix,
desired_mean_matrix= L+tau_matrix*observed.params$W+xBeta,
distribution='gaussian',
scalar_sigma=sqrt(sigma_squared))
tau_estimate_oracle <- treat.estimator(Y=Y, L.hat=L+xBeta, W=W)
mc_nnm_info <- matrix_completion_causal(Y=Y, W=W_document_missing, num_iter=1000, K=4,
lambda_grid=c(10^seq(-4,2,1), seq(2,5,1)),
tolerance=1e-04)
L_mc_nnm <- mc_nnm_info$L_hat
final.weight.matrix.normalized <- final.weight.matrix/sum(final.weight.matrix)
# it works, MAKE CROSS VALIDATION BETTER!! Needs to be adjusted to account for weighted sampling
# and what you are actually estimating.
## Account for sampling probability in cross-validation procedure, leading to different
## weight matrix
#final.weight.matrix
### Keep in mind that the weight matrix you feed in is not the exact matrix used for weighting (See paper)
r1comp_info <- completion_with_rank_estimation_validate_mu(Y=Y, W=W_document_missing,
weight_matrix = final.weight.matrix,
initial_rank=40,
tolerance=1e-04,
validation_max_iter=500,
min_iter=10,
max_iter=1000,
mu_grid=c(0, 10^seq(-2,3,1)),
K=5)
# c(0, 10^seq(-4, 3, 1))
mus.chosen[j] <- r1comp_info$chosen_mu
L_r1comp <- r1comp_info$L_hat
## Oracle in the sense that we know the untreated counterfactual
## What are we estimating now?? It is no longer L, because that is just a baseline matrix
## We are actually performing completion on Y (to get untreated counterfactuals)
## Why does using low-rank approximations work?
tau_estimate_mc_nnm <- treat.estimator(Y=Y, L.hat=L_mc_nnm, W=W)
tau_estimate_r1comp <- treat.estimator(Y=Y, L.hat=L_r1comp, W=W)
error_tau_mc_nnm <- mean(abs(tau_estimate_mc_nnm-delta_t)^2)
error_tau_r1comp <- mean(abs(tau_estimate_r1comp-delta_t)^2)
error_tau_oracle <- mean(abs(tau_estimate_oracle-delta_t)^2)
errors_this_L_mc_nnm[j] <- error_tau_mc_nnm
errors_this_L_r1comp[j] <- error_tau_r1comp
errors_this_L_oracle[j] <- error_tau_oracle
}
error_data <- data.frame(rbind(mse_and_se_of_mse(errors_this_L_mc_nnm),
mse_and_se_of_mse(errors_this_L_r1comp),
mse_and_se_of_mse(errors_this_L_oracle)), stringsAsFactors = F)
error_data <- cbind(c("MC NNM", "R1COMP", "ORACLE"),error_data )
names(error_data)[1] <- "Method"
effect_plot <- (ggplot(NULL, aes(x=1:length(delta_t), y=delta_t))
+ geom_point() + theme_bw() + xlab("Time") + ylab("Treatment Effect")
+ggtitle("True Treatment Effect Over Time"))
error_data
library(ggplot2)
library(LaplacesDemon)
library(glmnet)
library(foreach)
library(doParallel)
library(quadprog)
library(openxlsx)
library(stringr)
library(dplyr)
## derenski@msbjlr04.marshall.usc.edu:/home/derenski/causal.inference.propensity.scores/'server_simulation_tools_corrupted_control'/reports
## ./Desktop/USC/research/'Causal Inference with Corrupted Control Group'/server_simulation_tools_corrupted_control/parameters_and_compilation_code
## ## /home/josh/Desktop/USC/research/Causal Inference with Corrupted Control Group/server_simulation_tools_corrupted_control
## rsync -avh
server_name <- Sys.info()['nodename']
params <- read.xlsx("parameters_and_descriptions.xlsx",
sheet="parameter_data", rowNames = T)
source('causal_inference_methods_code_corrupted.R')
mse_and_se_of_mse <- function(error_mat){
squared_errors <- error_mat
the_mse <- mean(squared_errors)
se_mse <- sqrt((sd(squared_errors)^2)/prod(dim(squared_errors)))
final_stuff <- c(the_mse, se_mse)
names(final_stuff) <- c("mse", "se_mse")
return(final_stuff)
}
number_of_Es <- as.numeric(params["number_of_Es", ])
bootstrap_samps <- as.numeric(params["bootstrap_samps", ])
N <- as.numeric(params["N", ])
Time <- as.numeric(params["Time", ])
K <- as.numeric(params["K", ])
lengthCutoff <- as.numeric(params['lengthCutoff',])
matrix_type <- c("tall", 'wide')[(Time > N)+1]
Time0 <- as.numeric(params["Time0", ])
rho_parameter <- as.numeric(params["rho_parameter", ])
tau <- as.numeric(params["tau", ])
propTreat <- as.numeric(params["prop_treat", ])
sigma_squared <- as.numeric(params["sigma_squared", ])
min_iter <- as.numeric(params["min_iter", ])
max_iter <- as.numeric(params["max_iter", ])
tolerance <- as.numeric(params["tolerance", ])
L_scaling <- as.numeric(params["L_scaling", ])
arg_max <- as.numeric(params["arg_max", ])
y_max <- as.numeric(params["y_max", ])
halfway_time <-as.numeric( params["halfway_time", ])
cutoff <- as.numeric(params["cutoff", ])
design <- params["design", ]
all_function_names <- as.character(ls.str())
treatment_function_names <- all_function_names[str_detect(all_function_names, pattern="delta_t_")]
bs_method_names <- all_function_names[str_detect(all_function_names, pattern="bs_")]
prob_func_names <- all_function_names[str_detect(all_function_names, pattern="prob_(?!func)")]
max_lag <- as.numeric(params["max_lag", ])
group_gap <- as.numeric(params["group_gap", ])
desired_coverage <- as.numeric(params["desired_coverage", ])
treatment_function <- get(treatment_function_names[str_detect(treatment_function_names,
pattern=params["treatment_effect_function", ])])
bootstrap_method <- get(bs_method_names[str_detect(bs_method_names,
pattern=params["bootstrap_method", ])])
link_func <- params["link_func", ]
prob_func <- get(prob_func_names[str_detect(prob_func_names,
pattern=link_func)])
##################### Fixed Parameter Simulations
##################################################
###################################################
#################################################
mus.chosen <- rep(NA, number_of_Es)
betaEffect <- rep(10, K)
if (design=="staggered_adoption"){
observed.params <- parameter_maker_staggered(N=N, Time=Time, propTreat=propTreat,
linkFun='logit', K=K, lengthCutoff=lengthCutoff,
qFunc=qexp, qFuncParms = list(2))
# norm(observed.params$trueWeightMatrix-observed.params$weightMatrixEst, 'F')/sqrt(prod(dim(observed.params$weightMatrixEst)))
norm(observed.params$trueWeightMatrix-observed.params$weightMatrixEst, 'F')/sqrt(prod(dim(observed.params$weightMatrixEst)))
xBeta <- covariateEffectComponent(covArray=observed.params$covArray, betaEffect)
W <- observed.params$W
}else if (design=="block_treatment"){
# mean=c(-.5*group_gap, .5*group_gap)
observed.params <- parameter.maker_block(N=N, Time=Time, Time0=Time0, propTreat=propTreat,
link_func=link_func,
dist=rexp, prob_func = prob_func)
xBeta <- covariateEffectComponent(covArray=observed.params$covArray, betaEffect)
W <- observed.params$W
}
final.weight.matrix <- observed.params$weightMatrixEst
tau_matrix <- t(apply(W, MARGIN=1, FUN=treated_matrix_creator,
f_of_t=treatment_function, arg_max=arg_max,
y_max=y_max, halfway_time=halfway_time, cutoff=cutoff, value=tau))
control_units <- as.numeric(which(apply(W, MARGIN=1, FUN = function(x) all(x==0))))
pretreatment_times <- as.numeric(which(apply(W, MARGIN=2, FUN = function(x) all(x==0))))
treated_units <- as.numeric(which(apply(W, MARGIN=1, FUN = function(x) any(x==1))))
treatment_times <- as.numeric(which(apply(W, MARGIN=2, FUN = function(x) any(x==1))))
W_document_missing <- W
delta_t <- treatment_function(treatment_times-(min(treatment_times)-1),
arg_max=arg_max,
y_max=y_max, halfway_time=halfway_time, cutoff=cutoff, value=tau)
prediction_error_matrix_mc_nnm <- matrix(NA, nrow=1, ncol=number_of_Es)
prediction_error_matrix_r1comp <- matrix(NA, nrow=1, ncol=number_of_Es)
prediction_error_matrix_oracle <- matrix(NA, nrow=1, ncol=number_of_Es)
autocorrelation_matrix <- make_rho_mat(rho=rho_parameter, p=dim(W)[2])
sig_to_noise_ratios <- c()
desired_clusters <- 30
max_available_clusters <- detectCores()
cl <- makeCluster(min(c(max_available_clusters, desired_clusters))-3)
registerDoParallel(cl)
### Single run comparison
# set.seed(3729)
errors_this_L_mc_nnm <- rep(NA, number_of_Es)
errors_this_L_r1comp <- rep(NA, number_of_Es)
errors_this_L_oracle <- rep(NA, number_of_Es)
L <- matrix(5, nrow=N, ncol=Time)
for (j in 1:number_of_Es){
print(j)
Y <- nortaNoise(number=N, corr_mat=autocorrelation_matrix,
desired_mean_matrix= L+tau_matrix*observed.params$W+xBeta,
distribution='gaussian',
scalar_sigma=sqrt(sigma_squared))
tau_estimate_oracle <- treat.estimator(Y=Y, L.hat=L+xBeta, W=W)
mc_nnm_info <- matrix_completion_causal(Y=Y, W=W_document_missing, num_iter=1000, K=4,
lambda_grid=c(10^seq(-4,2,1), seq(2,5,1)),
tolerance=1e-04)
L_mc_nnm <- mc_nnm_info$L_hat
final.weight.matrix.normalized <- final.weight.matrix/sum(final.weight.matrix)
# it works, MAKE CROSS VALIDATION BETTER!! Needs to be adjusted to account for weighted sampling
# and what you are actually estimating.
## Account for sampling probability in cross-validation procedure, leading to different
## weight matrix
#final.weight.matrix
### Keep in mind that the weight matrix you feed in is not the exact matrix used for weighting (See paper)
r1comp_info <- completion_with_rank_estimation_validate_mu(Y=Y, W=W_document_missing,
weight_matrix = final.weight.matrix,
initial_rank=40,
tolerance=1e-04,
validation_max_iter=500,
min_iter=10,
max_iter=1000,
mu_grid=c(0, 10^seq(-2,3,1)),
K=5)
# c(0, 10^seq(-4, 3, 1))
mus.chosen[j] <- r1comp_info$chosen_mu
L_r1comp <- r1comp_info$L_hat
## Oracle in the sense that we know the untreated counterfactual
## What are we estimating now?? It is no longer L, because that is just a baseline matrix
## We are actually performing completion on Y (to get untreated counterfactuals)
## Why does using low-rank approximations work?
tau_estimate_mc_nnm <- treat.estimator(Y=Y, L.hat=L_mc_nnm, W=W)
tau_estimate_r1comp <- treat.estimator(Y=Y, L.hat=L_r1comp, W=W)
error_tau_mc_nnm <- mean(abs(tau_estimate_mc_nnm-delta_t)^2)
error_tau_r1comp <- mean(abs(tau_estimate_r1comp-delta_t)^2)
error_tau_oracle <- mean(abs(tau_estimate_oracle-delta_t)^2)
errors_this_L_mc_nnm[j] <- error_tau_mc_nnm
errors_this_L_r1comp[j] <- error_tau_r1comp
errors_this_L_oracle[j] <- error_tau_oracle
}
error_data <- data.frame(rbind(mse_and_se_of_mse(errors_this_L_mc_nnm),
mse_and_se_of_mse(errors_this_L_r1comp),
mse_and_se_of_mse(errors_this_L_oracle)), stringsAsFactors = F)
error_data <- cbind(c("MC NNM", "R1COMP", "ORACLE"),error_data )
names(error_data)[1] <- "Method"
effect_plot <- (ggplot(NULL, aes(x=1:length(delta_t), y=delta_t))
+ geom_point() + theme_bw() + xlab("Time") + ylab("Treatment Effect")
+ggtitle("True Treatment Effect Over Time"))
error_data
library(ggplot2)
library(LaplacesDemon)
library(glmnet)
library(foreach)
library(doParallel)
library(quadprog)
library(openxlsx)
library(stringr)
library(dplyr)
## derenski@msbjlr04.marshall.usc.edu:/home/derenski/causal.inference.propensity.scores/'server_simulation_tools_corrupted_control'/reports
## ./Desktop/USC/research/'Causal Inference with Corrupted Control Group'/server_simulation_tools_corrupted_control/parameters_and_compilation_code
## ## /home/josh/Desktop/USC/research/Causal Inference with Corrupted Control Group/server_simulation_tools_corrupted_control
## rsync -avh
server_name <- Sys.info()['nodename']
params <- read.xlsx("parameters_and_descriptions.xlsx",
sheet="parameter_data", rowNames = T)
source('causal_inference_methods_code_corrupted.R')
mse_and_se_of_mse <- function(error_mat){
squared_errors <- error_mat
the_mse <- mean(squared_errors)
se_mse <- sqrt((sd(squared_errors)^2)/prod(dim(squared_errors)))
final_stuff <- c(the_mse, se_mse)
names(final_stuff) <- c("mse", "se_mse")
return(final_stuff)
}
number_of_Es <- as.numeric(params["number_of_Es", ])
bootstrap_samps <- as.numeric(params["bootstrap_samps", ])
N <- as.numeric(params["N", ])
Time <- as.numeric(params["Time", ])
K <- as.numeric(params["K", ])
lengthCutoff <- as.numeric(params['lengthCutoff',])
matrix_type <- c("tall", 'wide')[(Time > N)+1]
Time0 <- as.numeric(params["Time0", ])
rho_parameter <- as.numeric(params["rho_parameter", ])
tau <- as.numeric(params["tau", ])
propTreat <- as.numeric(params["prop_treat", ])
sigma_squared <- as.numeric(params["sigma_squared", ])
min_iter <- as.numeric(params["min_iter", ])
max_iter <- as.numeric(params["max_iter", ])
tolerance <- as.numeric(params["tolerance", ])
L_scaling <- as.numeric(params["L_scaling", ])
arg_max <- as.numeric(params["arg_max", ])
y_max <- as.numeric(params["y_max", ])
halfway_time <-as.numeric( params["halfway_time", ])
cutoff <- as.numeric(params["cutoff", ])
design <- params["design", ]
all_function_names <- as.character(ls.str())
treatment_function_names <- all_function_names[str_detect(all_function_names, pattern="delta_t_")]
bs_method_names <- all_function_names[str_detect(all_function_names, pattern="bs_")]
prob_func_names <- all_function_names[str_detect(all_function_names, pattern="prob_(?!func)")]
max_lag <- as.numeric(params["max_lag", ])
group_gap <- as.numeric(params["group_gap", ])
desired_coverage <- as.numeric(params["desired_coverage", ])
treatment_function <- get(treatment_function_names[str_detect(treatment_function_names,
pattern=params["treatment_effect_function", ])])
bootstrap_method <- get(bs_method_names[str_detect(bs_method_names,
pattern=params["bootstrap_method", ])])
link_func <- params["link_func", ]
prob_func <- get(prob_func_names[str_detect(prob_func_names,
pattern=link_func)])
##################### Fixed Parameter Simulations
##################################################
###################################################
#################################################
mus.chosen <- rep(NA, number_of_Es)
betaEffect <- rep(10, K)
if (design=="staggered_adoption"){
observed.params <- parameter_maker_staggered(N=N, Time=Time, propTreat=propTreat,
linkFun='logit', K=K, lengthCutoff=lengthCutoff,
qFunc=qexp, qFuncParms = list(2))
# norm(observed.params$trueWeightMatrix-observed.params$weightMatrixEst, 'F')/sqrt(prod(dim(observed.params$weightMatrixEst)))
norm(observed.params$trueWeightMatrix-observed.params$weightMatrixEst, 'F')/sqrt(prod(dim(observed.params$weightMatrixEst)))
xBeta <- covariateEffectComponent(covArray=observed.params$covArray, betaEffect)
W <- observed.params$W
}else if (design=="block_treatment"){
# mean=c(-.5*group_gap, .5*group_gap)
observed.params <- parameter.maker_block(N=N, Time=Time, Time0=Time0, propTreat=propTreat,
link_func=link_func,
dist=rexp, prob_func = prob_func)
xBeta <- covariateEffectComponent(covArray=observed.params$covArray, betaEffect)
W <- observed.params$W
}
final.weight.matrix <- observed.params$weightMatrixEst
tau_matrix <- t(apply(W, MARGIN=1, FUN=treated_matrix_creator,
f_of_t=treatment_function, arg_max=arg_max,
y_max=y_max, halfway_time=halfway_time, cutoff=cutoff, value=tau))
control_units <- as.numeric(which(apply(W, MARGIN=1, FUN = function(x) all(x==0))))
pretreatment_times <- as.numeric(which(apply(W, MARGIN=2, FUN = function(x) all(x==0))))
treated_units <- as.numeric(which(apply(W, MARGIN=1, FUN = function(x) any(x==1))))
treatment_times <- as.numeric(which(apply(W, MARGIN=2, FUN = function(x) any(x==1))))
W_document_missing <- W
delta_t <- treatment_function(treatment_times-(min(treatment_times)-1),
arg_max=arg_max,
y_max=y_max, halfway_time=halfway_time, cutoff=cutoff, value=tau)
prediction_error_matrix_mc_nnm <- matrix(NA, nrow=1, ncol=number_of_Es)
prediction_error_matrix_r1comp <- matrix(NA, nrow=1, ncol=number_of_Es)
prediction_error_matrix_oracle <- matrix(NA, nrow=1, ncol=number_of_Es)
autocorrelation_matrix <- make_rho_mat(rho=rho_parameter, p=dim(W)[2])
sig_to_noise_ratios <- c()
desired_clusters <- 30
max_available_clusters <- detectCores()
cl <- makeCluster(min(c(max_available_clusters, desired_clusters))-3)
registerDoParallel(cl)
### Single run comparison
# set.seed(3729)
errors_this_L_mc_nnm <- rep(NA, number_of_Es)
errors_this_L_r1comp <- rep(NA, number_of_Es)
errors_this_L_oracle <- rep(NA, number_of_Es)
L <- matrix(5, nrow=N, ncol=Time)
for (j in 1:number_of_Es){
print(j)
Y <- nortaNoise(number=N, corr_mat=autocorrelation_matrix,
desired_mean_matrix= L+tau_matrix*observed.params$W+xBeta,
distribution='gaussian',
scalar_sigma=sqrt(sigma_squared))
tau_estimate_oracle <- treat.estimator(Y=Y, L.hat=L+xBeta, W=W)
mc_nnm_info <- matrix_completion_causal(Y=Y, W=W_document_missing, num_iter=1000, K=4,
lambda_grid=c(10^seq(-4,2,1), seq(2,5,1)),
tolerance=1e-04)
L_mc_nnm <- mc_nnm_info$L_hat
final.weight.matrix.normalized <- final.weight.matrix/sum(final.weight.matrix)
# it works, MAKE CROSS VALIDATION BETTER!! Needs to be adjusted to account for weighted sampling
# and what you are actually estimating.
## Account for sampling probability in cross-validation procedure, leading to different
## weight matrix
#final.weight.matrix
### Keep in mind that the weight matrix you feed in is not the exact matrix used for weighting (See paper)
r1comp_info <- completion_with_rank_estimation_validate_mu(Y=Y, W=W_document_missing,
weight_matrix = final.weight.matrix,
initial_rank=40,
tolerance=1e-04,
validation_max_iter=500,
min_iter=10,
max_iter=1000,
mu_grid=c(0, 10^seq(-2,3,1)),
K=5)
# c(0, 10^seq(-4, 3, 1))
mus.chosen[j] <- r1comp_info$chosen_mu
L_r1comp <- r1comp_info$L_hat
## Oracle in the sense that we know the untreated counterfactual
## What are we estimating now?? It is no longer L, because that is just a baseline matrix
## We are actually performing completion on Y (to get untreated counterfactuals)
## Why does using low-rank approximations work?
tau_estimate_mc_nnm <- treat.estimator(Y=Y, L.hat=L_mc_nnm, W=W)
tau_estimate_r1comp <- treat.estimator(Y=Y, L.hat=L_r1comp, W=W)
error_tau_mc_nnm <- mean(abs(tau_estimate_mc_nnm-delta_t)^2)
error_tau_r1comp <- mean(abs(tau_estimate_r1comp-delta_t)^2)
error_tau_oracle <- mean(abs(tau_estimate_oracle-delta_t)^2)
errors_this_L_mc_nnm[j] <- error_tau_mc_nnm
errors_this_L_r1comp[j] <- error_tau_r1comp
errors_this_L_oracle[j] <- error_tau_oracle
}
error_data <- data.frame(rbind(mse_and_se_of_mse(errors_this_L_mc_nnm),
mse_and_se_of_mse(errors_this_L_r1comp),
mse_and_se_of_mse(errors_this_L_oracle)), stringsAsFactors = F)
error_data <- cbind(c("MC NNM", "R1COMP", "ORACLE"),error_data )
names(error_data)[1] <- "Method"
effect_plot <- (ggplot(NULL, aes(x=1:length(delta_t), y=delta_t))
+ geom_point() + theme_bw() + xlab("Time") + ylab("Treatment Effect")
+ggtitle("True Treatment Effect Over Time"))
error_data
plot(table(rowSums(W)))
table(rowSums(W))
matrixThing <- matrix(1:9, nrow=3, ncol=3)
svd(matrixThing)
matrixThing <- matrix(1:2, nrow=2, ncol=3)
svd(matrixThing)
install.packages('devtools')
library(devtools)
install_github("jpvert/apg")
library(apg)
Y
X <- matrix(0, nrow=N, ncol=Time)
M <- W
W <- final.weight.matrix
lmbda <- 1
max_rank=NULL
min_value=NULL
max_value=NULL
apg_max_iter=100
apg_eps=1e-6
apg_use_restart=T
m = dim(X)[1]
n = dim(X)[2]
prox <- function(Z, t){
Z <- matrix(Z, nrow=m, ncol=n)
if (is.null(max_rank)){
svd_thing <- svd(Z)
U <- svd_thing$u
VT <- t(svd_thing$v)
S <- svd_thing$d
}else{
stop("max_rank can't be specified right now.")
}
S <- diag(np.maximum(S - lmbda*t, 0))
Z <- (U*S) %*% VT
if (!is.null(min_value)){
mask = Z < min_value
if (sum(mask) > 0){
Z[mask] = min_value
}
}
if (!is.null(max_value)){
mask = Z > max_value
if (sum(mask) > 0){
Z[mask] = max_value
}
}
return(as.numeric(Z))
}
M_one_mask <- (M == 1)
masked_weights = W[M_one_mask]
masked_X = X[M_one_mask]
grad <- function(Z){
grad <- matrix(0, nrow=m, ncol=n)
grad[M_one_mask] = (matrix(Z, nrow=m, ncol=n)[M_one_mask] - masked_X) * masked_weights
return(as.numeric(grad))
}
opts <- list(X_INIT=np.zeros(m*n), MAX_ITERS=apg_max_iter, EPS=apg_eps,
USE_GRA=T, USE_RESTART=apg_use_restart, QUIET=T)
opts <- list(X_INIT=matrix(0, nrow=m, ncol=n), MAX_ITERS=apg_max_iter, EPS=apg_eps,
USE_GRA=T, USE_RESTART=apg_use_restart, QUIET=T)
X_hat = apg(grad_f=grad, prox=prox, X_init=np.zeros(m*n),
max_iters=apg_max_iter,
eps=apg_eps,
use_gra=True,
use_restart=apg_use_restart,
quiet=True)
?apg
X_hat = apg(grad_f=grad, prox_h=prox, opts=opts)
X_hat = apg(grad_f=grad, prox_h=prox, dim_x=m*n, opts=opts)
X_hat = apg(grad_f=grad, prox_h=prox, dim_x=m*n, opts)
X_hat = apg(grad_f=grad, prox_h=prox, dim_x=m*n, list(X_INIT=matrix(0, nrow=m, ncol=n), MAX_ITERS=apg_max_iter, EPS=apg_eps,
USE_GRA=T, USE_RESTART=apg_use_restart, QUIET=T))
X_hat = apg(grad_f=grad, prox_h=prox, dim_x=m*n)
X_hat = apg(grad_f=grad, prox_h=prox, dim_x=m*n, X_INIT=matrix(0, nrow=m, ncol=n))
cat('\014')
X_hat = apg(grad_f=grad, prox_h=prox, dim_x=m*n, list(X_INIT=matrix(0, nrow=m, ncol=n), MAX_ITERS=apg_max_iter, EPS=apg_eps,
USE_GRA=T, USE_RESTART=apg_use_restart, QUIET=T))
cat('\014')
X_hat = apg(grad_f=grad, prox_h=prox, dim_x=m*n, opts=NULL)
X_hat = apg(grad_f=grad, prox_h=prox, dim_x=m*n, list(X_INIT=matrix(0, nrow=m, ncol=n), MAX_ITERS=apg_max_iter, EPS=apg_eps,
USE_GRA=T, USE_RESTART=apg_use_restart, QUIET=T))
cat('\014')
install.packages("reticulate")
library(reticulate)
file.choose()
file.choose()
source_python("/home/josh/GitHub/mnar_mc/mc_algorithms.py")
Y
source_python("/home/josh/GitHub/mnar_mc/mc_algorithms.py")
repl_python()
py_config()
library(reticulate)
repl_python()
