---
title: "Inference with Propensity Scores"
author: "Joshua Derenski"
output: pdf_document
params: 

  number_of_L: 1

  draws_per_L: 5
  
  bootstrap_samps: 100

  N: 500
  
  N1: 5

  Time: 20
  
  Time1: 6

  R: 3
  
  rho_parameter: 0

  tau: 10

  sigma_squared: 16

  penalized: True

  exchangable: F
  
  min_iter: 10
  
  max_iter: 1000

  tolerance: 1e-06

  error: 'gaussian'
  
  df: 1
  
  L_scaling: 5
  
  arg_max: 6
  
  y_max: 20
  
  halfway_time: 6
  
  cutoff: 3
  
  treatment_effect_function: "constant"
  
  design: "block_treatment"
  
  average_treatment_length: 6
  
  lag_structure: "random"
  
  max_lag: 4
  
  rank_estimation_method: "threshold"
  
  output_directory: "../reports"
---

```{r, echo=F, message=F, warning=F}

library(ggplot2)
library(LaplacesDemon)
library(glmnet)
library(foreach)
library(doParallel)
library(quadprog)

max_available_clusters <- detectCores()-1
  
desired_clusters <- 20
  
cl <- makeCluster(min(c(max_available_clusters, desired_clusters)))

registerDoParallel(cl)
 
source('causal_inference_methods_code_corrupted.R')


mse_and_se_of_mse <- function(error_mat){
  
  squared_errors <- error_mat
  
  the_mse <- mean(squared_errors)
  
  se_mse <- sqrt((sd(squared_errors)^2)/prod(dim(squared_errors)))
  
  final_stuff <- c(the_mse, se_mse)
  
  names(final_stuff) <- c("mse", "se_mse")
  
  return(final_stuff)
  
}

```

```{r, echo=F}

# 3729 = GOOD SEED

# set.seed(3729)

number_of_L <- params$number_of_L

draws_per_L <- params$draws_per_L

bootstrap_samps <- params$bootstrap_samps

N <- params$N

N0 <- N-params$N1

Time <- params$Time

Time0 <- Time-params$Time1

R <- params$R

rho_parameter <- params$rho_parameter

tau <- params$tau

sigma_squared <- params$sigma_squared

penalized <- params$penalized

exchangable <- params$exchangable

min_iter <- params$min_iter

max_iter <- params$max_iter

tolerance <- as.numeric(params$tolerance)

error = params$error

df <- params$df

rank_estimation_method <- params$rank_estimation_method

L_scaling <- params$L_scaling

arg_max <- params$arg_max
  
y_max <- params$y_max
  
halfway_time <- params$halfway_time

cutoff <- params$cutoff

design <- params$design

lag_structure <- params$lag_structure

average_treatment_length <- min(params$average_treatment_length, Time-Time0)

max_lag <- params$max_lag

treatment_function <- list_of_functions[[params$treatment_effect_function]]

```
# Simulations

## Parameters

- Number of Ls: `r number_of_L`

- Draws per L: `r draws_per_L`

- Number of Units: `r N`

- Number of Control Units: `r N0`

- Number of Times: `r Time`

- Number of pre-treatment Times: `r Time0`

- Rank of L: `r R`

- Autocorrelation Parameter: `r rho_parameter`

- True Effect Size for Constant Effect: `r tau`

- Error Type: `r error`

- Error Variance (if Gaussian error): `r sigma_squared`

- Degrees of freedom (if t-error): `r df`

- Exchangable: `r exchangable`

- Penalized: `r penalized`

- Rank Estimation Method: `r rank_estimation_method`

- Scaling for $L$: `r L_scaling`

- Treatment Effect Type: `r params$treatment_effect_function`

- Treatment Design: `r design`

- Lag Structure (if using staggered adoption structure): `r lag_structure`

- Average Treatment Length (if using staggered adoption structure, with random adoption): `r average_treatment_length`

- Maximum lag: `r max_lag`

# Integrating Propensity Scores into Simulation Design

We need to allow for propensity score information in our simulation setup. For the moment, we can assume we know the propensity scores. 


We can do a simple treatment design like this: 

1. Treatment can't happen until after a certain time period. 
2. After the time period treatment occurs with certain probability, dependent on unit-specific covariates (so the propensity score only changes with unit, not necessarily with time.)
3. Remember, propensity score-based weights are based off of different formulas! Issue: We cannot have any rows or columns with all 0 weights. 

Important: Make the treatment at previous time point a covariate! This will allow you to ensure treatment after initial treated period. 






# r1comp vs Competitors, Missing Control Cells
```{r, echo=F}

prop_miss <- 0

if (design=="staggered_adoption"){ ## Come up with a way to vary the lag in the staggered structure
  
  if(lag_structure == "random"){
    
    ones_we_make <- c(rep(0, N0), pmin(rpois(N-N0, 
                                            lambda=average_treatment_length-1)+1, 
                                      min(max_lag*(N-N0), .8*Time)))
    
  }else if (lag_structure=="constant"){ ## Does not control T-T0
    
    ones_we_make <- c(rep(0, N0), pmin(max_lag*seq(1, (N-N0)), floor(.8*Time)))
    
  }

}else if (design=="block_treatment"){
  
  observed.params <- parameter.maker(N=N, Time=Time, Time0=Time0, dist=rnorm, mean=c(-1,1))
  
  x <- observed.params$x
    
  W <- observed.params$W
  
}

final.weight.matrix <- observed.params$final.weight.matrix

tau_matrix <- t(apply(W, MARGIN=1, FUN=treated_matrix_creator, 
                      f_of_t=treatment_function, arg_max=arg_max, 
                      y_max=y_max, halfway_time=halfway_time, cutoff=cutoff))

control_units <- as.numeric(which(apply(W, MARGIN=1, FUN = function(x) all(x==0))))

pretreatment_times <- as.numeric(which(apply(W, MARGIN=2, FUN = function(x) all(x==0))))

treated_units <- as.numeric(which(apply(W, MARGIN=1, FUN = function(x) any(x==1))))
  
treatment_times <- as.numeric(which(apply(W, MARGIN=2, FUN = function(x) any(x==1))))

W_document_missing <- W

delta_t <- treatment_function(treatment_times-(min(treatment_times)-1),
                              arg_max=arg_max, 
                      y_max=y_max, halfway_time=halfway_time, cutoff=cutoff, value=tau)

prediction_error_matrix_mc_nnm <- matrix(NA, nrow=number_of_L, ncol=draws_per_L)

prediction_error_matrix_r1comp <- matrix(NA, nrow=number_of_L, ncol=draws_per_L)

prediction_error_matrix_oracle <- matrix(NA, nrow=number_of_L, ncol=draws_per_L)

autocorrelation_matrix <- make_rho_mat(rho=rho_parameter, p=dim(W)[2])

sig_to_noise_ratios <- c()

for (i in 1:number_of_L){
  
 # set.seed(3729)

  errors_this_L_mc_nnm <- rep(NA, draws_per_L)
  
  errors_this_L_r1comp <- rep(NA, draws_per_L)
  
  errors_this_L_oracle <- rep(NA, draws_per_L)
  
  L <- matrix(5, nrow=N, ncol=Time)
  
  for (j in 1:draws_per_L){
    
    print(j)
    
    cov.mat <- t(sapply(observed.params$x, FUN=function(x) rep(x, each=Time)))

    Y <- norta(number=N, corr_mat=autocorrelation_matrix,
                 desired_mean_matrix= L+tau_matrix*observed.params$W+5*cov.mat, distribution='gaussian',
                 scalar_sigma=sqrt(sigma_squared))
    
    if (N-N0 > 1){
    
    treatment_subjects_averaged <- colMeans(Y[1:dim(Y)[1] > N0,])
    
    W_averaged <- colMeans(W[1:dim(W)[1] > N0,])
    
    new_Y <- rbind(Y[1:dim(Y)[1] <= N0,], treatment_subjects_averaged)
    
    new_W <- rbind(W[1:dim(Y)[1] <= N0,], W_averaged)
    
    } else {
    
      new_Y <- Y
    
      new_W <- W
      
      
    }

    
    mc_nnm_info <- matrix_completion_causal(Y=Y, W=W_document_missing, num_iter=1000, K=4, 
                            lambda_grid=c(10^seq(-4,2,1), seq(2,5,1)),
                            tolerance=1e-04)

    
    L_mc_nnm <- mc_nnm_info$L_hat
    
    final.weight.matrix.normalized <- final.weight.matrix/sum(final.weight.matrix)
    

    
    # it works, MAKE CROSS VALIDATION BETTER!! Needs to be adjusted to account for weighted sampling
    # and what you are actually estimating.
    
    ## Account for sampling probability in cross-validation procedure, leading to different 
    ## weight matrix
    
    r1comp_info <- completion_with_rank_estimation_validate_mu(Y=Y, W=W_document_missing,  
                                                        weight_matrix = final.weight.matrix,
                                                        initial_rank=40,
                                                        tolerance=1e-03, 
                                                        min_iter=10,
                                                        max_iter=1000,
                                                        mu_grid=100,
                                                        K=2)
    
    L_r1comp <- r1comp_info$L_hat 

    ## Oracle in the sense that we know the untreated counterfactual
    
    ## What are we estimating now?? It is no longer L, because that is just a baseline matrix
    
    ## We are actually performing completion on Y (to get untreated counterfactuals)
    
    ## Why does using low-rank approximations work? 
    
    tau_estimate_oracle <- treat.estimator(Y=Y, L.hat=L, W=W)
  
    tau_estimate_mc_nnm <- treat.estimator(Y=Y, L.hat=L_mc_nnm, W=W)
    
    tau_estimate_r1comp <- treat.estimator(Y=Y, L.hat=L_r1comp, W=W)
    
    
    error_tau_mc_nnm <- mean(abs(tau_estimate_mc_nnm-delta_t)^2)
    
    error_tau_r1comp <- mean(abs(tau_estimate_r1comp-delta_t)^2)
    
    error_tau_oracle <- mean(abs(tau_estimate_oracle-delta_t)^2)

    errors_this_L_mc_nnm[j] <- error_tau_mc_nnm
    
    errors_this_L_r1comp[j] <- error_tau_r1comp
    
    errors_this_L_oracle[j] <- error_tau_oracle

  }
  
  prediction_error_matrix_mc_nnm[i,] <- errors_this_L_mc_nnm
  
  prediction_error_matrix_r1comp[i,] <- errors_this_L_r1comp
  
  prediction_error_matrix_oracle[i,] <- errors_this_L_oracle

}


effect_plot <- (ggplot(NULL, aes(x=1:length(delta_t), y=delta_t)) 
                + geom_point() + theme_bw() + xlab("Time") + ylab("Treatment Effect")
                +ggtitle("True Treatment Effect Over Time"))


```


# RMSE: r1comp

```{r, echo=F}
mean(prediction_error_matrix_r1comp)
```

# RMSE: MC NNM

```{r, echo=F}
mean(prediction_error_matrix_mc_nnm)
```

# RMSE: Oracle

```{r, echo=F}
mean(prediction_error_matrix_oracle)
```



```{r, echo=F}
stopCluster(cl)
```





# Testing Bootstrap Ideas

```{r, echo=F, eval=F}

N <- 5000

treatment_effect <- 300

B <- 1000

B_trials <- 500

effect_distribution_regular <- rep(NA, B)

effect_distribution_weighted <- rep(NA, B)

(mean(  (observed_data/combined_group_scores)* actually_treated)
    -mean(  (observed_data/(1-combined_group_scores)* (1-actually_treated))))

interval_lengths_regular <- rep(NA, B_trials)

interval_lengths_weighted <- rep(NA, B_trials)


captured_regular <- rep(NA, B_trials)

captured_weighted <- rep(NA, B_trials)


for (B_trial in 1:B_trials){
  
  beta_parameters <-rbind(c(1,9), c(c(7,4)))

  probably_untreated_group <- rnorm(N/2)

  probably_treated_group <- rnorm(N/2, mean=10)

  probably_untreated_group_scores <- rbeta(N/2, shape1=beta_parameters[1,1], shape2=beta_parameters[1,2])

  probably_treated_group_scores <- rbeta(N/2, shape1=beta_parameters[2,1], shape2=beta_parameters[2,2])

  combined_groups <- c(probably_untreated_group, probably_treated_group)

  combined_group_scores <- c(probably_untreated_group_scores, probably_treated_group_scores )

  actually_treated <- as.numeric(rbernoulli(N,combined_group_scores))

  observed_data <- rnorm(N, mean=combined_groups+treatment_effect*actually_treated)
  
  for (b in 1:B){
    
    
  ## With regular bootstrap sampling, weighting by propensity  
  
    b_sample <- sample.int(N, size=N, prob=rep(1/N,N), replace=T)
    
    effect_distribution_regular[b] <- (mean(  (observed_data[b_sample]/combined_group_scores[b_sample])* actually_treated[b_sample]) -mean(  (observed_data[b_sample]/(1-combined_group_scores[b_sample])* (1-actually_treated[b_sample]))))
    
    
  ## With propensity-weighted bootstrap  
    
    b_sample_treated <- sample(which(as.logical(actually_treated)), size=sum(actually_treated), 
                                   replace = T, 
    prob =(1/combined_group_scores[which(as.logical(actually_treated))])/
      sum(1/combined_group_scores[which(as.logical(actually_treated))]) )
    
    b_sample_untreated <- sample(which(as.logical(1-actually_treated)), size=sum(1-actually_treated), 
                                   replace = T, 
                                   prob =(1/(1-combined_group_scores[as.logical(1-actually_treated)]))
                                   /sum(1/(1-combined_group_scores[as.logical(1-actually_treated)])) )
  
    effect_distribution_weighted[b] <- mean(observed_data[b_sample_treated])-mean(observed_data[b_sample_untreated])
    
  }

  ci_regular <- unname(quantile(effect_distribution_regular, c(.025, .975)))
    
  ci_weighted <- unname(quantile(effect_distribution_weighted, c(.025, .975)))
    
    
  interval_lengths_regular[B_trial] <- diff(unname(ci_regular))
  
  interval_lengths_weighted[B_trial] <- diff(unname(ci_weighted))
  
  captured_regular[B_trial] <- (ci_regular[1] < treatment_effect & treatment_effect < ci_regular[2])
  
  captured_weighted[B_trial] <- (ci_weighted[1] < treatment_effect & treatment_effect < ci_weighted[2])
  
  print(B_trial)

}


#truehist(effect_distribution_weighted)
#truehist(effect_distribution_regular)


mean(effect_distribution_regular)
mean(effect_distribution_weighted)

mean(captured_regular, na.rm=T)
sd(captured_regular)/sqrt(B_trials)

mean(captured_weighted, na.rm=T)
sd(captured_weighted)/sqrt(B_trials)
```
In short, it is better to bootstrap in the traditional way and save the weighting for the regression than to do a weighted bootstrap based on propensity scores. It appears that the second way underestimates the variance.



# Bootstrap Testing, unit level method
```{r, echo=F, eval=F}

prop_miss <- 0

if (design=="staggered_adoption"){ ## Come up with a way to vary the lag in the staggered structure
  
  if(lag_structure == "random"){
    
    ones_we_make <- c(rep(0, N0), pmin(rpois(N-N0, 
                                            lambda=average_treatment_length-1)+1, 
                                      min(max_lag*(N-N0), .8*Time)))
    
  }else if (lag_structure=="constant"){ ## Does not control T-T0
    
    ones_we_make <- c(rep(0, N0), pmin(max_lag*seq(1, (N-N0)), floor(.8*Time)))
    
  }

}else if (design=="block_treatment"){
  
  ones_we_make <- c(rep(0, N0), rep(Time-Time0, N-N0))
  
}

W <- W_maker(N=N, Time=Time, ones_per_row = ones_we_make)

tau_matrix <- t(apply(W, MARGIN=1, FUN=treated_matrix_creator, 
                      f_of_t=treatment_function, arg_max=arg_max, 
                      y_max=y_max, halfway_time=halfway_time, cutoff=cutoff))

control_units <- as.numeric(which(apply(W, MARGIN=1, FUN = function(x) all(x==0))))

pretreatment_times <- as.numeric(which(apply(W, MARGIN=2, FUN = function(x) all(x==0))))

treated_units <- as.numeric(which(apply(W, MARGIN=1, FUN = function(x) any(x==1))))
  
treatment_times <- as.numeric(which(apply(W, MARGIN=2, FUN = function(x) any(x==1))))

control_cells <- which(W==0)

cells_to_delete <- sample(c(2,0), size=length(control_cells), prob=c(prop_miss, 1-prop_miss), replace=T)

W_document_missing <- W

W_document_missing[control_cells] <- cells_to_delete

delta_t <- treatment_function(treatment_times-(min(treatment_times)-1),
                              arg_max=arg_max, 
                      y_max=y_max, halfway_time=halfway_time, cutoff=cutoff, value=tau)

prediction_error_matrix_mc_nnm <- matrix(NA, nrow=number_of_L, ncol=draws_per_L)

prediction_error_matrix_r1comp <- matrix(NA, nrow=number_of_L, ncol=draws_per_L)

prediction_error_matrix_oracle <- matrix(NA, nrow=number_of_L, ncol=draws_per_L)

autocorrelation_matrix <- make_rho_mat(rho=rho_parameter, p=dim(W)[2])

sig_to_noise_ratios <- c()

for (i in 1:number_of_L){
  
 # set.seed(3729)

  
  errors_this_L_mc_nnm <- rep(NA, draws_per_L)
  
  errors_this_L_r1comp <- rep(NA, draws_per_L)
  
  errors_this_L_oracle <- rep(NA, draws_per_L)
  
  if (exchangable){
    
    U_vec <- rexp(n=N*R, rate=1)

    V_vec <- rexp(n=Time*R, rate=1)
  
    U <- matrix(U_vec, nrow=N, ncol=R, byrow=T)

    V <- matrix(V_vec, nrow=Time, ncol=R, byrow=T)
  
  }else{
    
    U <- matrix(NA, nrow=N, ncol=R, byrow=T)

    V <- matrix(NA, nrow=Time, ncol=R, byrow=T)
    
    for (row_unit in 1:N){
      
      U[row_unit,] <- rpois(n=R, lambda=sqrt(row_unit/N))
      
    } 
    
    for (row_time in 1:Time){
      
      V[row_time,] <- rpois(n=R, lambda=sqrt(row_time/Time))
      
    }
    
  }

  L <- L_scaling*(U %*% t(V))
  
  if (error == 'gaussian'){
      
      Y <- norta(number=N, corr_mat=autocorrelation_matrix,
                 desired_mean_matrix= L+tau_matrix*W, distribution='gaussian',
                 scalar_sigma=sqrt(sigma_squared))
      
    } else if (error == 't'){
    
      Y <- norta(number=N, corr_mat=autocorrelation_matrix,
                 desired_mean_matrix= L+tau_matrix*W,
                 distribution='t', scalar_sigma=sqrt(sigma_squared))
    
    } else if (error == 'poisson'){
      
      if (exchangable == F){
      
        L <- abs(L)+1
        
      }
      
      Y <- norta(number=N, corr_mat=autocorrelation_matrix,
                 desired_mean_matrix= L+tau_matrix*W,
                 distribution='poisson', scalar_sigma=sqrt(sigma_squared))
      
    } else if (error == 'scaled_gamma'){
      
      if (exchangable == F){
      
        L <- abs(L)+1
        
      }
      
      Y <- norta(number=N, corr_mat=autocorrelation_matrix,
                 desired_mean_matrix= L+tau_matrix*W,
                 distribution='scaled_gamma', scalar_sigma=sqrt(sigma_squared))
      
    }else if (error == 'exponential'){
      
      if (exchangable == F){
      
        L <- abs(L)+1
        
      }
      
      Y <- norta(number=N, corr_mat=autocorrelation_matrix,
                 desired_mean_matrix= L+tau_matrix*W,
                 distribution='exponential', scalar_sigma=sqrt(sigma_squared))

    }
    
    if (N-N0 > 1){
    
    treatment_subjects_averaged <- colMeans(Y[1:dim(Y)[1] > N0,])
    
    W_averaged <- colMeans(W[1:dim(W)[1] > N0,])
    
    new_Y <- rbind(Y[1:dim(Y)[1] <= N0,], treatment_subjects_averaged)
    
    new_W <- rbind(W[1:dim(Y)[1] <= N0,], W_averaged)
    
    } else {
    
      new_Y <- Y
    
      new_W <- W
      
      
    }
  
  
  bootstrap.estimates <- matrix(NA, nrow=bootstrap_samps, ncol=max(rowSums(W)))
  
  ## Bootstrap at unit level
  
  is_it_control <- apply(W, MARGIN=1, FUN=function(x) all(x==0))
  
  control_units <- which(is_it_control)
  
  treatment_units <- which(!is_it_control)
  
  ## bootstrap at cell level 
  
  for (j in 1:bootstrap_samps){
    
    print(j)
    
    control_b_samp <- sample(control_units, size=length(control_units),
                             replace=T)
    
    treatment_b_samp <- sample(treatment_units, size=length(treatment_units),
                             replace=T)
    
    Y.bsamp <- Y[c(control_b_samp, treatment_b_samp), ]
    
    W.bsamp <- W[c(control_b_samp, treatment_b_samp), ]
    
  #  mc_nnm_info <- matrix_completion_causal(Y=Y.bsamp, W=W.bsamp, num_iter=1000, K=4, 
  #                          lambda_grid=c(10^seq(-4,2,1), seq(2,5,1)),
  #                          tolerance=1e-04)
    
  #  L_mc_nnm <- mc_nnm_info$L_hat
    
    r1comp_info <- completion_with_rank_estimation_validate_mu(Y=Y.bsamp, W=W.bsamp,  
                                                        initial_rank=30,
                                                        tolerance=1e-04, 
                                                        min_iter=10,
                                                        max_iter=1000,
                                                        mu_grid=10^seq(0, 2,1),
                                                        K=4)
    
    L_r1comp <- r1comp_info$L_hat

    ## Oracle in the sense that we know L

  #  tau_estimate_oracle <- treat.estimator(Y=Y, L.hat=L, W=W)
    #  tau_estimate_mc_nnm <- treat.estimator(Y=Y, L.hat=L_mc_nnm, W=W)
      
    bootstrap.estimates[j, ] <- treat.estimator(Y=Y.bsamp, L.hat=L_r1comp, W=W)
      
 

  }
  
  delta.t.estimate <- colMeans(bootstrap.estimates, na.rm=T)
  
  se.delta.t.estimate <- apply(bootstrap.estimates, 2, FUN=function(x) sd(x, na.rm=T))
  
  lower.cis <- apply(bootstrap.estimates, MARGIN=2, FUN=function(x) quantile(x,.025, na.rm=T))
  
  upper.cis <- apply(bootstrap.estimates, MARGIN=2, FUN=function(x) quantile(x,.975, na.rm=T))
  
}

```


# $\delta_{t}$ Estimator Simulations


```{r, eval=F}

  number.tries <- 500

  effect.estimates <- matrix(NA, nrow=number.tries, ncol=Time-Time0)

  naive.estimates <- matrix(NA, nrow=number.tries, ncol=Time-Time0)
  
  observed.params <- parameter.maker(N=N, Time=Time, Time0=Time0, dist = rnorm, mean=c(3,-3))
  
  x <- observed.params$x
    
  W <- observed.params$W

  final.weight.matrix <- observed.params$final.weight.matrix
  
  tau_matrix <- t(apply(W, MARGIN=1, FUN=treated_matrix_creator, 
                      f_of_t=treatment_function, arg_max=arg_max, 
                      y_max=y_max, halfway_time=halfway_time, cutoff=cutoff))

  control_units <- as.numeric(which(apply(W, MARGIN=1, FUN = function(x) all(x==0))))

  pretreatment_times <- as.numeric(which(apply(W, MARGIN=2, FUN = function(x) all(x==0))))

  treated_units <- as.numeric(which(apply(W, MARGIN=1, FUN = function(x) any(x==1))))
  
  treatment_times <- as.numeric(which(apply(W, MARGIN=2, FUN = function(x) any(x==1))))

  delta_t <- treatment_function(treatment_times-(min(treatment_times)-1),
                              arg_max=arg_max, 
                      y_max=y_max, halfway_time=halfway_time, cutoff=cutoff, value=tau)

  autocorrelation_matrix <- make_rho_mat(rho=rho_parameter, p=dim(W)[2])
  
  for (j in 1:number.tries){
    
    the.params <- parameter.maker(N=N, Time=Time, Time0=Time0, dist = rnorm, mean=c(3,-3))
    
    L <- matrix(5, nrow=N, ncol=Time)
    
    cov.mat <- t(sapply(the.params$x, FUN=function(x) rep(x, each=Time)))

    Y <- norta(number=N, corr_mat=autocorrelation_matrix,
                 desired_mean_matrix= L+tau_matrix*the.params$W+2*cov.mat, distribution='gaussian',
                 scalar_sigma=sqrt(sigma_squared))
    
    if (N-N0 > 1){
    
    treatment_subjects_averaged <- colMeans(Y[1:dim(Y)[1] > N0,])
    
    W_averaged <- colMeans(W[1:dim(W)[1] > N0,])
    
    new_Y <- rbind(Y[1:dim(Y)[1] <= N0,], treatment_subjects_averaged)
    
    new_W <- rbind(W[1:dim(Y)[1] <= N0,], W_averaged)
    
    } else {
    
      new_Y <- Y
    
      new_W <- W
      
    }

    effect.estimates[j,] <- treat.estimator((Y*the.params$W)*the.params$final.weight.matrix,
                                    (Y*(1-the.params$W)*the.params$final.weight.matrix), W)
    
    naive.estimates[j,] <- treat.estimator(Y*the.params$W,
                                    Y*(1-the.params$W), W)
    
  }
  
  
colMeans(effect.estimates)

colMeans(naive.estimates)


# mean(Y[W==1])-mean(Y[W==0])
```









```{r, eval=F}
max_available_clusters <- detectCores()-1
  
desired_clusters <- 20
  
cl <- makeCluster(min(c(max_available_clusters, desired_clusters)))

registerDoParallel(cl)

stopCluster(cl)



```


# Useful Commands For Server Uploading

1. scp -r "/Users/joshuaderenski/Desktop/USC/research/Causal Inference with Corrupted Control Group/server_simulation_tools_corrupted_control/" derenski@msbjlr04.marshall.usc.edu:/home/derenski/causal.inference.propensity.scores


2. scp "/Users/joshuaderenski/Desktop/USC/research/Causal Inference with Corrupted Control Group/server_simulation_tools_corrupted_control/parameters_and_compilation_code/parameters_and_descriptions.xlsx" derenski@msbjlr04.marshall.usc.edu:/home/derenski/causal.inference.propensity.scores/server_simulation_tools_corrupted_control/parameters_and_compilation_code/



3. scp "/Users/joshuaderenski/Desktop/USC/research/Causal Inference with Corrupted Control Group/server_simulation_tools_corrupted_control/parameters_and_compilation_code/propensity.score.simulations.R" derenski@msbjlr04.marshall.usc.edu:/home/derenski/causal.inference.propensity.scores/server_simulation_tools_corrupted_control/parameters_and_compilation_code/propensity.score.simulations.R

4. scp "/Users/joshuaderenski/Desktop/USC/research/Causal Inference with Corrupted Control Group/server_simulation_tools_corrupted_control/parameters_and_compilation_code/causal_inference_methods_code_corrupted.R" derenski@msbjlr04.marshall.usc.edu:/home/derenski/causal.inference.propensity.scores/server_simulation_tools_corrupted_control/parameters_and_compilation_code/


5. rsync -avh derenski@msbjlr04.marshall.usc.edu:/home/derenski/causal.inference.propensity.scores/server_simulation_tools_corrupted_control/reports "/Users/joshuaderenski/Desktop"


