{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import ast\n",
    "import apgpy\n",
    "from sklearn.utils.extmath import randomized_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.Rhistory',\n",
       " 'parameters_and_descriptions.xlsx',\n",
       " '.~lock.parameters_and_descriptions.xlsx#',\n",
       " 'python data.json',\n",
       " 'causal_inference_methods_code_corrupted.R',\n",
       " 'Survival Analysis Simulation Tools.R',\n",
       " 'corrupted_control_simulations.Rmd',\n",
       " 'thing.json',\n",
       " 'reports',\n",
       " 'Untitled.ipynb',\n",
       " '.ipynb_checkpoints',\n",
       " 'for_compiling_report.R',\n",
       " 'Survival Data Generation.R',\n",
       " 'propensity.score.simulations.R']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_logistic_function(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def grad_std_logistic_function(x):\n",
    "    z = np.exp(-x)\n",
    "    return z / (1 + z)**2\n",
    "\n",
    "\n",
    "def mod_logistic_function(x, gamma, one_minus_logistic_gamma):\n",
    "    x = np.clip(x, -gamma, gamma)\n",
    "    return 1 / (1 + np.exp(-x)) + .5 * (1 + x/gamma) * one_minus_logistic_gamma\n",
    "\n",
    "\n",
    "def grad_mod_logistic_function(x, gamma, one_minus_logistic_gamma):\n",
    "    x = np.clip(x, -gamma, gamma)\n",
    "    z = np.exp(-x)\n",
    "    return z / (1 + z)**2 + one_minus_logistic_gamma / (2*gamma)\n",
    "\n",
    "\n",
    "def one_bit_MC_fully_observed(M, link, link_gradient, tau, gamma, max_rank=None,\n",
    "                              apg_max_iter=100, apg_eps=1e-12,\n",
    "                              apg_use_restart=True):\n",
    "    # parameters are the same as in the paper; if `max_rank` is set to None,\n",
    "    # then exact SVD is used\n",
    "    m = M.shape[0]\n",
    "    n = M.shape[1]\n",
    "    tau_sqrt_mn = tau * np.sqrt(m*n)\n",
    "\n",
    "    def prox(_A, t):\n",
    "        _A = _A.reshape(m, n)\n",
    "\n",
    "        # project so nuclear norm is at most tau*sqrt(m*n)\n",
    "        if max_rank is None:\n",
    "            U, S, VT = np.linalg.svd(_A, full_matrices=False)\n",
    "        else:\n",
    "            U, S, VT = randomized_svd(_A, max_rank)\n",
    "        nuclear_norm = np.sum(S)\n",
    "        if nuclear_norm > tau_sqrt_mn:\n",
    "            S *= tau_sqrt_mn / nuclear_norm\n",
    "            _A = np.dot(U * S, VT)\n",
    "\n",
    "        # clip matrix entries with absolute value greater than gamma\n",
    "        mask = np.abs(_A) > gamma\n",
    "        if mask.sum() > 0:\n",
    "            _A[mask] = np.sign(_A[mask]) * gamma\n",
    "\n",
    "        return _A.flatten()\n",
    "\n",
    "    M_one_mask = (M == 1)\n",
    "    M_zero_mask = (M == 0)\n",
    "    def grad(_A):\n",
    "        _A = _A.reshape(m, n)\n",
    "\n",
    "        grad = np.zeros((m, n))\n",
    "        grad[M_one_mask] = -link_gradient(_A[M_one_mask])/link(_A[M_one_mask])\n",
    "        grad[M_zero_mask] = \\\n",
    "            link_gradient(_A[M_zero_mask])/(1 - link(_A[M_zero_mask]))\n",
    "\n",
    "        return grad.flatten()\n",
    "\n",
    "    A_hat = apgpy.solve(grad, prox, np.zeros(m*n),\n",
    "                        max_iters=apg_max_iter,\n",
    "                        eps=apg_eps,\n",
    "                        use_gra=True,\n",
    "                        use_restart=apg_use_restart,\n",
    "                        quiet=True)\n",
    "    P_hat = link(A_hat.reshape(m, n))\n",
    "    return P_hat\n",
    "\n",
    "\n",
    "def one_bit_MC_mod_fully_observed(M, link, link_gradient, tau, gamma,\n",
    "                                  max_rank=None, apg_max_iter=100,\n",
    "                                  apg_eps=1e-12, apg_use_restart=True,\n",
    "                                  phi=None):\n",
    "    # parameters are the same as in the paper; if `max_rank` is set to None,\n",
    "    # then exact SVD is used\n",
    "    m = M.shape[0]\n",
    "    n = M.shape[1]\n",
    "    tau_sqrt_mn = tau * np.sqrt(m*n)\n",
    "    M_zero_mask = (M == 0)\n",
    "    if phi is None:\n",
    "        phi = .95 * gamma\n",
    "\n",
    "    def prox(_A, t):\n",
    "        _A = _A.reshape(m, n)\n",
    "\n",
    "        # project so nuclear norm is at most tau*sqrt(m*n)\n",
    "        if max_rank is None:\n",
    "            U, S, VT = np.linalg.svd(_A, full_matrices=False)\n",
    "        else:\n",
    "            U, S, VT = randomized_svd(_A, max_rank)\n",
    "        nuclear_norm = np.sum(S)\n",
    "        if nuclear_norm > tau_sqrt_mn:\n",
    "            S *= tau_sqrt_mn / nuclear_norm\n",
    "            _A = np.dot(U * S, VT)\n",
    "\n",
    "        # clip matrix entries with absolute value greater than gamma\n",
    "        mask = np.abs(_A) > gamma\n",
    "        if mask.sum() > 0:\n",
    "            _A[mask] = np.sign(_A[mask]) * gamma\n",
    "\n",
    "        mask = _A[M_zero_mask] > phi\n",
    "        if mask.sum() > 0:\n",
    "            _A[M_zero_mask][mask] = phi\n",
    "\n",
    "        return _A.flatten()\n",
    "\n",
    "    M_one_mask = (M == 1)\n",
    "    def grad(_A):\n",
    "        _A = _A.reshape(m, n)\n",
    "\n",
    "        grad = np.zeros((m, n))\n",
    "        grad[M_one_mask] = -link_gradient(_A[M_one_mask])/ \\\n",
    "            link(np.maximum(_A[M_one_mask], -gamma))\n",
    "        grad[M_zero_mask] = \\\n",
    "            link_gradient(_A[M_zero_mask])/ \\\n",
    "            (1 - link(np.minimum(_A[M_zero_mask], phi)))\n",
    "\n",
    "        return grad.flatten()\n",
    "\n",
    "    A_hat = apgpy.solve(grad, prox, np.zeros(m*n),\n",
    "                        max_iters=apg_max_iter,\n",
    "                        eps=apg_eps,\n",
    "                        use_gra=True,\n",
    "                        use_restart=apg_use_restart,\n",
    "                        quiet=True)\n",
    "    P_hat = link(A_hat.reshape(m, n))\n",
    "    return P_hat\n",
    "\n",
    "\n",
    "def weighted_softimpute(X, M, W, lmbda, max_rank=None,\n",
    "                        min_value=None, max_value=None,\n",
    "                        apg_max_iter=100, apg_eps=1e-6,\n",
    "                        apg_use_restart=True):\n",
    "    # if `max_rank` is set to None, then exact SVD is used\n",
    "    m = X.shape[0]\n",
    "    n = X.shape[1]\n",
    "\n",
    "    def prox(Z, t):\n",
    "        Z = Z.reshape(m, n)\n",
    "\n",
    "        # singular value shrinkage\n",
    "        if max_rank is None:\n",
    "            U, S, VT = np.linalg.svd(Z, full_matrices=False)\n",
    "        else:\n",
    "            U, S, VT = randomized_svd(Z, max_rank)\n",
    "        S = np.maximum(S - lmbda*t, 0)\n",
    "        Z = np.dot(U * S, VT)\n",
    "\n",
    "        # clip values\n",
    "        if min_value is not None:\n",
    "            mask = Z < min_value\n",
    "            if mask.sum() > 0:\n",
    "                Z[mask] = min_value\n",
    "        if max_value is not None:\n",
    "            mask = Z > max_value\n",
    "            if mask.sum() > 0:\n",
    "                Z[mask] = max_value\n",
    "\n",
    "        return Z.flatten()\n",
    "\n",
    "    M_one_mask = (M == 1)\n",
    "    masked_weights = W[M_one_mask]\n",
    "    masked_X = X[M_one_mask]\n",
    "    def grad(Z):\n",
    "        grad = np.zeros((m, n))\n",
    "        grad[M_one_mask] = (Z.reshape(m, n)[M_one_mask] - masked_X) * masked_weights\n",
    "        return grad.flatten()\n",
    "\n",
    "    X_hat = apgpy.solve(grad, prox, np.zeros(m*n),\n",
    "                        max_iters=apg_max_iter,\n",
    "                        eps=apg_eps,\n",
    "                        use_gra=True,\n",
    "                        use_restart=apg_use_restart,\n",
    "                        quiet=True).reshape((m, n))\n",
    "    return X_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"python data.json\"\n",
    "\n",
    "with open(file_path) as f:\n",
    "    data = json.loads(f.read())\n",
    "\n",
    "dataInDict = ast.literal_eval(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "yArray = np.array(dataInDict['Y'])\n",
    "\n",
    "W = np.array(dataInDict['W'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "thingus = one_bit_MC_fully_observed(M=W, link=std_logistic_function, link_gradient=grad_std_logistic_function, \n",
    "                                    tau=9, gamma=100, max_rank=None,\n",
    "                              apg_max_iter=100, apg_eps=1e-12,\n",
    "                              apg_use_restart=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00221071, 0.00221071, 0.00221071, ..., 0.00221071, 0.00221071,\n",
       "        0.00221071],\n",
       "       [0.00221071, 0.00221071, 0.00221071, ..., 0.00221071, 0.00221071,\n",
       "        0.00221071],\n",
       "       [0.00221071, 0.00221071, 0.00221071, ..., 0.00221071, 0.00221071,\n",
       "        0.00221071],\n",
       "       ...,\n",
       "       [0.00221071, 0.00221071, 0.00221071, ..., 0.00221071, 0.00221071,\n",
       "        0.00221071],\n",
       "       [0.00221071, 0.00221071, 0.00221071, ..., 0.00221071, 0.00221071,\n",
       "        0.00221071],\n",
       "       [0.00221071, 0.00221071, 0.00221071, ..., 0.00221071, 0.00221071,\n",
       "        0.00221071]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thingus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
